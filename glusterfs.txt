    GlusterFS

распределденная ФС без поддержки болчных и объектных хранилищ. 
каждый сервер кластера будет являться хранящие данные нодом, тут
нет отдельных серверов которые обеспечивают согласованность данных,
ее жизнедеятельность... это универсальное решение, это сетевая ФС,
хорошо интегрируется с кубернетес
brick - кирпичик данный - созданные локальные ФС на соответствующих
разделах, которые используются драйвером glusterfs как строительные
блоки для создания распределенной ФС, для хранения части ее данных/
(ФС - файловая система)

пример:
используем 6 серверов с centos 7
на каждом имеем диск на 25Гб, что в сумме даст 150гб - /dev/vdb

 1. подготовка
поднимаем LVM  (см на строчные буквы) на каждом сервере

pvcreate /dev/vdb

vgcreate vg_bricks /dev/vdb

lvcreate -L 10G -n lv_brick1 vg_bricks
lvcreate -L 10G -n lv_brick2 vg_bricks
lvcreate -l 100%free -n lv_brick3 vg_bricks

форматируем каждый раздел LVM и создаем на них ФС - XFS

mkfs.xfs -i size=512 /dev/vg_bricks/lv_brick1
mkfs.xfs -i size=512 /dev/vg_bricks/lv_brick2
mkfs.xfs -i size=512 /dev/vg_bricks/lv_brick3

монтируем эти ФС к нашей корневой ФС, создаем директории
к которым будем монтировать

mkdir -p /bricks/brick1 /bricks/brick2 /bricks/brick3

чтобы это все монтировалось автоматически при запуске компьютера
нужно добавить записи в fstab
(тут показан не совсем хороший пример, т.к. берем для монтирования
символьное имя, а нужно брать UUID)

nano /etc/fstab
/dev/vg_bricks/lv_brick1 /bricks/brick1 xfs rw,noatime,inode64,nouuid 1 2
/dev/vg_bricks/lv_brick2 /bricks/brick2 xfs rw,noatime,inode64,nouuid 1 2
/dev/vg_bricks/lv_brick3 /bricks/brick3 xfs rw,noatime,inode64,nouuid 1 2

теперь монтируем это все (без перезагрузки)

mount -a 

и можно посмотреть, что получилось

df -hT

у нас должны бысть строчки:
/dev/mapper/vg_bricks-lv_brick1 xfs  .....    /bricks/brick1
/dev/mapper/vg_bricks-lv_brick2 xfs  .....    /bricks/brick2
/dev/mapper/vg_bricks-lv_brick3 xfs  .....    /bricks/brick3

в каждой созданной нами ФС создадим пустую директорию с которой
драйвер будет непосредственно работать

mksir /bricks/brick1/brick /bricks/brick2/brick /bricks/brick3/brick

 2. теперь, когда все подготовили - ставим glusterfs

ищем и устанавливаем репозитории

yum search centos-release-gluster

устанавливаем 7 версию ФС

yum install centos-release-gluster7

устанавливаем непосредственно пакеты ФС

yum install glusterfs glusterfs-libs glusterfs-server

после установки запускаем сервис glusterd и добавляем его в автозагрузку

systemctl enable glusterd
systemctl start glusterd

смотрим статус - должен быть активен (зеленый)

systemctl status glusterd

ТЕПЕРЬ ВСЕ ЭТО ДЕЛАЕМ НА ОСТАВШИХСЯ 5 СЕРВЕРАХ

 3. тома
теперь о томах - на "кирпичики" нельзя сохранять данные, нужно 
объеденить находящиеся на разных машинах кирпичики и создать
на их основе том, который будет отвечать за запись данных, 
синхронизацию между серверами и предоставление всех гарантий по
обеспечению отказоустойчивости. монтирование так же осуществляется
через том. пользователю эти кирпичики не доступны.

тома могут быть:
distributed - простое объеденение в линейное пространство - максимально
              доступное место, резервирования нет

replicated - всего навсего настраивает репликацию между определенным
             кол-вом кирпичиков входящих в том, не очень удачное решение
             если у нас 1000 серверов, то мы получим 1000 реплик
             одной и той же информации. может пригодиться при 
             использовании  с кубирнетес

distributed replicated  - комбинация первых двух, нужен для 
                          горизонтального масштабирования, тут
                          множество блоков реплик которые следуют
                          непрерывной цепочной, и таким образом,
                          общее дисковое пространство линейно 
                          увеличивается с кол-вом серверов в нашем пуле.
                          используется для хранения информации
                          которую нельзя терять.

dispersed    - тут можно снизить накладные расходы на отказоустойчивость 
               при этом сохраняя все гарантии, что она предоставляет.
               эта модель использует:
               из N кирпичиков, некоторое кол-во выделяется для хранения
               избыточной информации некоего erasure codes (стирающий
               код). имея этот код, мы может, в случае отказа кирпичика,
               восстановить данные используя этот сохраненный код. 
               нужно выделить один кирпичик для хранения стирающего кода,
               и можно потерять один кирпичик с данными без потери 
               информации, если выделим два, то можем потерять два, т.е.
               два сервера могут быть выключены одновременно. похоже на
               raid5 или raidz/raidz2. тут мы жертвуем производительностью
               - при изменения данных на любом кирпичике входящем в пул, 
               ФС вынуждена пересоздать стирающий код для этого блока                     данных, а это в свою очередь означает, что необходимо
               перечитать данные всех кирпичиков и чем больше их входит
               в пул, тем больше будет нагрузка на систему

distributed dispersed  - этот тип тома бесполезен для горизонтального
                         масштабирования ФС, лучше когда хранится
                         важная информация, но она 
                         больше читается, а не записывается и кол-во
                         операций записи относительно не большое, 
                         тут пожем потерять один сервер из 
                         каждого дисперстного блока


смотрим что есть на серверах (пока ничего нет)
gluster pool list

объеденяем в тома в кластеры - тут 1 сервер и 2 сервер
gluster peer probe 192.168.122.52

ip -- второго сервера, можно использовать и DNS имена


теперь можно посмотреть на двух серверах что они объеденились
(вывод будет симметричным)

gluster pool list

добавляем остальные сервера в наш пул

gluster peer probe 192.168.122.53
gluster peer probe 192.168.122.54
gluster peer probe 192.168.122.55
gluster peer probe 192.168.122.56

 4.0 создаем том distributed ("удлинняем диск")
gluster volume created gluster-volume3 192.168.122.51:/bricks/brick3/brick 192.168.122.52:/bricks/brick3/brick 192.168.122.53:/bricks/brick3/brick 192.168.122.54:/bricks/brick3/brick 192.168.122.55:/bricks/brick3/brick 192.168.122.56:/bricks/brick3/brick 

смотрим на параметры созданного нами тома
gluster volume info

смотрим на сл значения 
type: distribute     -- показывает какой у нас том
number of bricks: 6  -- сколько "кирпичиков" данных входит в том
staus: created       --создан, но пока не запущен, после запуска
                       будет отображаться started

запускаем том
gluster volume start gluster-volume3

все создано, теперь это нужно примонтировать на другой компьютер,
чтобы все работало нужно установить пакет

ищем этот пакет

packman -Q | grep gluster

packman --программа в арх линуксе для поиска этого пакета

создадим директорию для монтрования нашего тома на локальную машину

mkdir /mnt/gluster-volume3

и теперь нушу ФС сюда монтируем, можно выбрать любой сервер ( наш ip)

mount -t glusterfs 192.168.122.51:/gluster-volume3 /mnt/gluster-volume3/

192.168.122.51  --сервер на котором создан этот том

сейчас, если посмотреть все объемы дисков

df -hT

то увидим, что 192.168.122.51:/gluster-volume3   30Гиг, что 6 кирпичиков
по 5 гиг каждый и дает такой рез

теперь в /mnt/gluster-volume3/ мы можем помещать любые файлы (туда
примонтировали) и все файлы будут распределены по нодам нашего кластера

 4.1 создаем том replicated (делаем 6 реплик одного и того же)

gluster volume create gluster-volume1 replica 6 192.168.122.51:/bricks/brick1/brick 192.168.122.52:/bricks/brick1/brick 192.168.122.53:/bricks/brick1/brick 192.168.122.54:/bricks/brick1/brick 192.168.122.55:/bricks/brick1/brick 192.168.122.56:/bricks/brick1/brick

replica 6   --метка о кол-ве необходимых реплик, ее можно задать например
              равную 3 или 4, тут будет уменьшено кол-во реплик и 
              увеличено дисковое пространство под хранение

но если мы будем объеденять всего 2 тома и делать 2 реплики - будет 
ошибка, хотя и сработает, но нужно будет подтвердит. проблема может
возникнуть из-за неполадки на сетях и получится SPLIT-BRAIN - это
когда несколько нодов одновременно будут считать себя главными и при 
восстановлении соединения они не смогут прийти к пониманию, данные какого
из них являются истинными, а какие необходимо перезаписать. возникновение
такой ситуации очень мала, но риски будут всегда. glusterfs имеет механизм
ухода (исправления) от такой ситуации, но только НЕ С ДВУМЯ репликами, тут
ФС не смодет гарантировать доступность. нужно использовать минимум три
леплики или добавлять специальный кирпичик для арбитра, который в случае
проблем, будет использоваться для решения split-brain ситуации (этот
кирпичик очень маленький и почти не занимает места)


том создается когда кол-во реплик равно кол-ву используемых кирпичиков
тут из 60 гиг только 10 гиг будут использоваться для хранения, остальные
будут дублировать данные

смотрим на параметры созданного нами тома
gluster volume info

смотрим на сл значения 
type: replicate              -- показывает какой у нас том
number of bricks: 1 x 6 = 6  -- сколько "кирпичиков" данных входит в том
staus: created               -- создан, но пока не запущен, после запуска
                                будет отображаться started


 4.2 создаем том distributed replicated
тут replica должна делиться без остатка на кол-во кирпичиков в системе,
иначе будет ошибка. так же ВАЖЕН ПОРЯДОК ПЕРЕЧИСЛЕНИЯ КИРПИЧИКОВ,
в нашем примере, каждые три будут составлять один блок репликации, а 
следовательно, если мы хотим получить настоящ отказоустойчивость,
мы должны убедиться, что каждый кирпичик в одном блоке находится на
сервере в разной зоне доступности.

gluster volume create gluster-volume1 replica 3 192.168.122.51:/bricks/brick1/brick 192.168.122.52:/bricks/brick1/brick 192.168.122.53:/bricks/brick1/brick 192.168.122.54:/bricks/brick1/brick 192.168.122.55:/bricks/brick1/brick 192.168.122.56:/bricks/brick1/brick

тут replica 3 
мы получаем два последовательно идущих блока репликации, каждый
их которых имеет фактор репликации три, вот первый блок
192.168.122.51:/bricks/brick1/brick 192.168.122.52:/bricks/brick1/brick 192.168.122.53:/bricks/brick1/brick
это три реплики которые будут входить в в первый такой блок 

вот второй блок и три реплики которые в него входят
192.168.122.54:/bricks/brick1/brick 192.168.122.55:/bricks/brick1/brick 192.168.122.56:/bricks/brick1/brick

смотрим чего у нас тут теперь

gluster volume info gluster-volume1

type: distributed-replicate  -- показывает какой у нас том
number of bricks: 2 x 3 = 6  -- два блока репликации по три реплики в 
                                каждом

если будет 1200 серверов, то получим 400 х 3 = 1200

staus: created               -- создан, но пока не запущен, после запуска
                                будет отображаться started


теперь опять примонтируем его к ФС
для начала созадим директорию

mkdir /mnt/gluster-volume1

и запустим команду монтирования

mount -t glusterfs 192.168.122.51:/gluster-volume1 /mnt/gluster-volume1

поскольку у нас тут две реплики и каждая имеет фактор репликации ТРИ, то
только 20 гиг из 60 гиг у нас будет доступно для использования 



 4.3 создаем том distributed dispersed

gluster volume create gluster-volume2 disperse 3 redundancy 1 192.168.122.51:/bricks/brick2/brick 192.168.122.52:/bricks/brick2/brick 192.168.122.53:/bricks/brick2/brick 192.168.122.54:/bricks/brick2/brick 192.168.122.55:/bricks/brick2/brick 192.168.122.56:/bricks/brick2/brick


disperse 3    --том типа disperse из трех реплик

redundancy 1  --одна из реплик является redundancy 111111111!!!!!!дописать

смотрим

gluster volume info gluster-volume2

вывод:

type: distributed-disperse       -- показывает какой у нас том
number of bricks: 2 x (2+1) = 6  -- два дисперстных блока, каждый из 
                                    которых из которых имеет 3 кирпичика,
                                    один из которых используется для
                                    хранения избыточной информации. каждый
                                    такой блок это 66% используемого
                                    дискогово пространства, значит мы 
                                    имеем 40 гиг из 60 гиг для хранения
                                    наших данных. можно и увеличить это
                                    значение, за счет снижения 
                                    производительности кластера

staus: created               -- создан, но пока не запущен, после запуска
                                будет отображаться started

 особенности:

мы можем напрямую читать/изменять файлы в кирпичике, но в этом случае
ФС ничего не будет знать об изменениях и в отдельно взятом кирпичике
может лежать часть данных. работать с файлами нужно только через
примонтированную директорию посредством драйвера ФС.

пробуем сохранить файл размером 6 гиг на кирпичик в 5 гиг:
при сохранении в ФС файла размером больше чем размер одного кирпичика 
используемого на уровне тома, мы получим ошибку записи (но запишим
макс возможного, просто не допишем) - по умолчанию файлы не могут 
выходить за рамки одного кирпичика, даже если на соседнем еще 
достаточно места

решение: 
активировать на любом из доступных томов режим sharding, который будет 
все файлы поступающие на хранение делить на маленькие кусочки и эти
кусочки будут реплицироваться между кирпичиками нашего кластера.
так же шардирование может сбалансировать нагрузку между нодами кластера.
чтобы активировать:

gluster volume set gluster-volume3 feature.shard enable

и укажем размер одного блока шардирования

gluster volume set gluster-volume3 feature.shard-block-size 64MB

смотрим

gluster volume info gluster-volume3

вывод:

feature.shard-block-size 64MB
feature.shard enable


воводим ноды из эксплуатации и вводим новые:
например, если у нашего третьего тома, где нет отказоустойчивости
необходимо вывести один сервер из эксплуатации и не повредить на нем
данные

запуск вывода из тома (удаляем кирпичик):
(должно быть достаточно свободного места, чтобы принять данные с 
даляемого кирпичика (ноды)), после доступный для записи 
размер - уменьшится

переходим на ноду
смотрим статус

gluster volume status gluster-volume3

выводит состояние всех нод и всех кирпичиков, у них статус online пока
везде Y

удаляем кирпичик
но на самом деле пока не удаляем, он не удалиться, будет все еще
логически связан, чтобы в случае чего - вернуть все назад

gluster volume remove-brick gluster-volume3 192.168.122.56:/bricks/brick3/brick start

gluster-volume3                      --выбраный том
192.168.122.56:/bricks/brick3/brick  --удаляемый кирпичик
start                                --старт удаления

смотрим состояние

gluster volume status gluster-volume3

некоторое время будет in progress, нужно дождаться состояния completed

более детальную информацию (сколько ошибок, сколько скопировано, 
пропущено) можно посмотреть так

gluster volume remove-brick gluster-volume3 192.168.122.56:/bricks/brick3/brick status


чтобы физически его удалить, закрепив произведеную операцию, нужно сделать commit:

gluster volume remove-brick gluster-volume3 192.168.122.56:/bricks/brick3/brick commit


теперь удалим целый блок репликации:
(тут у нас 2 блока репликации по 3 корпичика в каждом)
удалять отдельно кирпичики нельзя, нужно блоком!
(доступый для использования (записи) размер тоже уменьшится)

gluster volume remove-brick gluster-volume1 replica 3 192.168.122.51:/bricks/brick1/brick 192.168.122.52:/bricks/brick1/brick 192.168.122.53:/bricks/brick1/brick start

так же нужно смотреть статус, т.к. если данных и/или блоков репликации много - может уйти много времени, иногда дни, ФС то доступна и работает

gluster volume status gluster-volume1

и

gluster volume remove-brick gluster-volume1 replica 3 192.168.122.51:/bricks/brick1/brick 192.168.122.52:/bricks/brick1/brick 192.168.122.53:/bricks/brick1/brick status

завершения удаления (фиксирвание), после того, как все "переносы" 
выполнены

gluster volume remove-brick gluster-volume1 replica 3 192.168.122.51:/bricks/brick1/brick 192.168.122.52:/bricks/brick1/brick 192.168.122.53:/bricks/brick1/brick commit

теперь этот том сотоит из одного тома репликации в который входят три
реплики, соответственно и размер хранилища уменьшился (в 2 раза, мы
же отключили половину)


ДОБАВИТЬ КИРПИЧИК В КЛАСТЕР
(добавим тот, который ранее удалили)
gluster volume add-brick gluster-volume3 192.168.122.56:/bricks/brick3/brick force

force  --вынуждены использовать этот флаг, т.к. добавляемый кирпичик
         ранее был использован и ФС нас предупредит об ошибке, которую
         мы можем совершить и удалить данные

ЗАМЕНИТЬ ОДИН ИЗ КИРПИЧИКОВ НА ЛЕТУ ОСТАВШЕЙСЯ РЕПЛИКИ в томе
gluster-volume1 на один из кирпичиков который мы изъяли из него ранее
(gluster-volume1 - там где убрали один том репликации)

gluster volume replace-brick gluster-volume1 192.168.122.54:/bricks/brick1/brick 192.168.122.51:/bricks/brick1/brick commit force

(кирпичик на 192.168.122.54 заменяем на 192.168.122.51)



сейчас возьмем оставшиеся кирпичики, сформируем один блок репликации и
добавим этот блок к тому gluster-volume1

gluster volume add-brick gluster-volume1 replica3 192.168.122.54:/bricks/brick1/brick 192.168.122.52:/bricks/brick3/brick 192.168.122.53:/bricks/brick1/brick force


REBALANCE
gluster volume rebalance gluster-volume1 start

gluster volume rebalance gluster-volume1 status


ЗАМЕНА ДИСКА НА НОВЫЙ, ПУСТОЙ - ДОБАВЛЕНИЕ В КЛАСТЕР
будем считать, что после замены uuid нода (верояно все же диска)
не поменяется и ip адр ноды не поменяется и все входящие в кластер
ноды будут считать его все еще частью общего пула
(чтобы сервис после перезагрузки (включения) не поднялся - отключим
автозагрузку, а после перезагрузим (выкл и включим)

systemctl disable glusterd

reboot

теперь отмонтируем наши разделы

umount /bricks/brick*

и переформатируем ФС

mkfs.xfs -i size=512 /dev/vg_bricks/lv_brick1 -f
mkfs.xfs -i size=512 /dev/vg_bricks/lv_brick2 -f
mkfs.xfs -i size=512 /dev/vg_bricks/lv_brick3 -f

теперь их просто монтируем, т.к. они уже прописаны в sftab

mount -a

создадим поддиректории

mkdir /bricks/brick1/brick /bricks/brick2/brick /bricks/brick3/brick


"запускаем ноду" (сервис ФС)

systemctl start glusterd

добавляем его в автозагрузку

systenctl enable glusterd


Чтобы ДРАЙВЕР ПЕРЕПОДКЛЮЧИЛ (ВОССТАНОВИЛ) КАЖДЫЙ КИРПИЧИК находящийсв на этой ноде. Тут указан один кирпичик ДВА РАЗА, так и должно быть
gluster volume reset-brick gluster-volume1 192.168.122.52:/bricks/brick1/brick 192.168.122.52:/bricks/brick1/brick commit



чтобы небыло проблем при монтировании на сервер (ноду) который неожиданно
выключен - нужно УКАЗАТЬ РЕЗЕРВНУЮ НОДУ. тут все еще указываем главной нодой первый (192.168.122.51) сервер, но помимо него мы указали три других сервера которые стоит использовать в случае если главный сервер окажется недоступен (передается через параметр backup-volfile-servers=)
 
mount -t glusterfs -o backup-volfile-servers=192.168.122.51:192.168.122.52:192.168.122.53  192.168.122.51:/gluster-volume2  /mnt/gluster-volume2/



Рекомендовано использовать для gluster
nfs-ganesha



так же (для клиента)
mount -t glusterfs -obackup-volfile-server=<server2>:<server3>:...:<serverN> <server1>:/<volname> <mountpoint>

for i in $(seq 200 210); do dd if=/dev/urandom/ of=/mnt/gluster-volume1/random@i bs=1M count=10; done




ВЫВЕСТИ ВСЕ UUID ВСЕХ ВХОДЯЩИХ В КЛАСТЕР НОД

gluster pool list

ЗАПУСТИТЬ ТОМ

gluster volume start gluster-volume1


ПЕРЕСОЗДАТЬ ТОМА на ранее используемых кирпичиках нужно использовать
флаг force

gluster volume create gluster-volume1 replica 3 192.168.122.51:/bricks/brick1/brick 192.168.122.52:/bricks/brick1/brick 192.168.122.53:/bricks/brick1/brick 192.168.122.54:/bricks/brick1/brick 192.168.122.55:/bricks/brick1/brick 192.168.122.56:/bricks/brick1/brick force 


УДАЛИТЬ ТОМ

gluster volume delete gluster-volume1















