KVM

https://www.youtube.com/watch?v=tLQ2PtSVr58


смотрим, поддерживается ли аппаратная виртуализация
cat /proc/cpuinfo | grep -E '(vmx|svm)' -m l

vmx   -- виртуализация интел
svm   -- виртуализация амд

если ничего не отобразилось - значит аппаратную виртуализацию процессор не поддерживает  или в биос аппаратная виртуализация отключена

НА УБУНТУ  16.04

apt install qemu-kvm libvirt-bin virt-manager ovmf

virt-manager    -- если на десктопной версии убунту
ovmf    --  эмуляция uefi

добавим пользователя в группу libvirtd
сначала смотрим, есть ли группа
cat /etc/group | grep libvirt

добавляем в группу пользователя
adduser metelkov libvirtd

НАСТРАИВАЕМ ХОСТ
проверим фаервол
iptables -vnL
ничего не должно быть закрыто - система то чистая, а после того, как мы активируем интерфейс в виртуалке - у нас появятся некоторые правила

ИЗ КОНСОЛИ
virsh   -- переходим в виртуал шелл

настройки сети
net-  и двойной таб - покажет все комманды сети

net-list  --покажет все активные сетевые адаптеры которые сейчас настроены

net-list --all  -- увидим неактивную (в начале) дефолтную сеть

net-info default -- информация по сети default

net-start default  --запускаем подсеть default
по умолчанию подсеть 192.168.122.0

для перенастройки
net-edit default -- имя сети default
первый раз, нас спросят какой редактор мы будем использовать - выбрать nano
после мы провалимся в конфиг (в формате xml)
например поменяем адрес
ip address='192.168.123.1' netmask='255.255.255.0'
dhcp
range start='192.168.123.2' end='192.168.123.254'

сохраняем и выходим
после стартуем сеть
net-start default

чтобы сеть автоматически стартовала после перезагрузки нужно
net-autostart default  -- название сети default

чтобы сеть остановить
net-destroy default  -- название сети default

когда эта сеть остановлена в реальной машине тоже исчезнут упоминания о ней и исчезнут цепочки в iptables

создать свою сеть, используя сеть default как шаблон

смотрим сам шаблон
net-dumpxml default

теперь выходим из виртуалки и в реальной машине набираем
virsh net-dumpxml default  --имя сети drfault

перенаправим вывод (для копирования)
virsh net-dumpxml default > /root/default.xml (имя файла м.б. произвольное)

после редактируем этот файл
nano /root/default.xml
в нем меняем 
<name>vlan01</name>

UUID стираем, его нам присвоят автоматически

<bridge name='virbr1' stp='on' delay='0'/>

mac удаляем - присвоится новый

ip address='192.168.124.1' netmask='255.255.255.0'

dhcp
range start='192.168.124.2' end='192.168.124.254'

сохраняем, выходим из nano

переходим в virsh

комманды для создания сети
net-create и net-define

создадим сеть из нашего файла
net-define /root/default.xml

сеть появится в списке (при net-list --all)
мы можем эту сеть запустить

остановить сеть
net-destroy vlan01

удаляем полностью сеть (уничтажаем)
net-undefine vlan01


net-create --создает временную сеть не прописывая ее в конф файлах хоста
net-create /root/default.xml
но тут сразу получим статус ACTIVE и PERSISTENS = NO

разрушаем сеть
net-destroy vlan01

эта комманда не только остановит сеть, но и уничтожит ее сразу - неужно быть внимательнее при создании через net-create
а при 
net-undefine vlan01 - вывалится ошибка, что у конф файлах такой сети нет


НАСТРОЙКА ПУЛОВ, ГДЕ БУДУТ ХРАНИТЬСЯ ВИРТУАЛЬНЫЕ МАШИНЫ
НО для этого нужно СНАЧАЛА запустить нашу default сеть

pool-list -all  --покажет наши пулы
чтобы иметь пулы - нужно отдельное место - или отдельный диск или отдельный раздел

(для хранениия данных виртуальных машин)
создать дополнительное пространство данных - в этом пуле выбрать (предварительно создав) наш диск  (kvm1-disk2)
смотрим на наши (виртуальные) диски - наш новый диск /dev/vdb

разметим его
parted /dev/vdb

делаем новую разметку диска
(parted) mklabel msdos

выбираем едениц. измерения мегабайт
unit mib

и создадим первый раздел
mkpart
пусть будет primary  (p)

тип файловой системы, пока не важно
и выбираем размер - с первого мб по 8гиг

и таким же образом создаем еще один раздел
mkpart
primary (p)
file system ...
start (начало разметки) - см чуть выше инф о созданном предыдущ разделе
7629MiB 
end (конечное значение) -1  --все оставшееся свободное место на диске

выходим из parted

есть 2 формата хранения данных:
1 COW (COPY ON WRITE) - подходит для работы на ноутбуке или для тестовой вируалки (лабы)  - метод хранения сильно напоминает 
метод хранения virtualBox'а - в отдельной папке находится один большой файл - куда записываются все данные с виртуальной машины

2 RAW - сырой вариант - подходит для серверов, чтобы хранить большое кол-во данных различных виртуальных машин - использует как весь диск, 
так и раздел диска, работает на разных файловых системах и может использвать lvm.


чуть ранее мы разбили один дискк на два раздела, для фомата RAW - будем использовать lvm - сделаем это на vdb1

pvcreate /dev/vdb1
vgcreate vg0 /dev/vdb1

далее никаких логических томов делать не нужно, когда мы будем подключать пул - просто укажем нашу lvm группу и она будет 
использоваться kvm для хранения дисков


второй раздел - для диска формата COW - на нем просто создадим раздел
mkfs.ext4 /dev/vdb2

после создадим отдельный каталог
mkdir /mnt/kvm-disks

и смонтируем его туда
mount /dev/vdb2 /mnt/kvm-disks

переходим в virsh
virsh

подключаем два наших пула
для начала смотрим, что пулов нет
pool-list --all

первым создадим пул формата COW
есть два варианта
pool-create-as  и pool-create
pool-define-as  и pool-define

pool-create так же как и pool-define требует XML файла - если его нет, то сщздастся временный пул - при отключении (pool-destroy) - этот пул удалится

pool-create-as  и  pool-define-as  - создает конфигурационный файл на нашем хосте


начнем
pool-create-as 

подключим директорию

pool-create-as --name local --type dir --target /mnt/kvm-disks/ (указали подмонтированный раздел)

смотрим
pool-list
-----------
local  active  autostart=no

т.е. пул создался и уже активен - можно создать виртуальный диск в этой области.

удалить пул
pool-destroy local 

смотрим
pool-list

и тут ничего не будет

теперь создадим через
pool-define-as

pool-define-as --name local --type dir --target /mnt/kvm-disks
--name local --имя пула
--type dir --тип - директория
--target /mnt/kvm-disks  -- куда будет смонтирован пул

смотрим
pool-list
и ничего нет, потомучто пул не активен

чтобы его активировать
pool-start local
пул стал активан на autostart=no, т.е. при перезагрузке хоста он не будет стартовать

pool-autostart local
вот теперь будет подключаться при перезагрузке хоста

теперь нужно внести запись в fstab, чтобы диск монтировался автоматически
nano /etc/fstab

тут тоже 2 варианта - или путь /dev/vdb  или  UUID
UUID предпочтительнее, т.к. не зависит от пути и не меняется

смотрим UUID у /dev/vdb2
blkid /dev/vdb2

чтобы не кописпстить ДОЗАПИШЕМ его в fstab
blkid /dev/vdb2 | cut -d ' ' -f 2 >> /etc/fstab

!ВНИМАНИЕ НА >> !
ПРЕДВАРИТЕЛЬНО СДЕЛАТЬ КОПИЮ FSTAB

после опять редактируем fstab
nano /etc/fstab
UUID у нас там уже есть, допишим некоторые параметры
UUID=_наш_uuid_ /mnt/kvm-disks ext4 default 0 0
дописать эти параметры
/mnt/kvm-disks -- путь     ext4  --файловая система   default 0 0  --так надо

проверяем
virsh
pool-list
нам покажет, что раздел local активный и autostart=yes


мы его сейчас остановим  (переведем в неактивное срстояне)
pool-destroy local

размонтируем диск
umount /mnt/kvm-disks

и монтируем через fstab
mount /mnt/kvm-disks

проверим монтирование
df -h

должны  увидеть
/dev/vdb2  (наш путь)  /mnt/kvm-disks  (куда подмонтирован)

вернемся в virsh
virsh

и запустим пул
pool-start local

вот таким образом используется пул по ринципу "директория" - механизм как в virtualBox


ПОДКЛЮЧАЕМ ПУЛ НА LVM
pool-define-as  (создаем на длительное время)

pool-define-as --name vg0 --type logical --source-name vg0
--name vg0   --имя, пусть называется как и наша логическая группа
--type logical   --тип, обязательно logical!
--source-name vg0  --наименование lvm группы, которую создали ранее

если посмотрим пулы - пул будет не активен
pllo-list --all
vg0  inactive  autostart=no

делаем активным и автостарт
pool-start vg0  -- активный
pool-autostart vg0  --автостарт=yes


ПОДКЛЮЧИВ ПУЛЫ МЫ МОЖЕМ СОЗДАВАТЬ ДИСКИ ДЛЯ ВИРТУАЛЬНЫХ МАШИН
vol-  и два таба

создаем первый виртуальный диск
vol-create-as --pool vg0 --name test --capacity 1G
--pool vg0  --укажем пул в котором хотим создавать - наша vg0 группа (в ней)
--name test  --название диска - будет test
--capacity 1G   --выделяемый объем - 1Гб

смотрим диски
vol-list vg0

vg0  --обязательно указывать в каком поле хотим смотреть

lvdisplay теперь покажет, что диск создан
LV PATH = /dev/vg0/test

виртуальная мошина будет считать это отдельным блочным устройством (отдельный диск) и будет писать информацию туда

ДЛЯ УДАЛЕНИЯ ДИСКА
vol-delete test --pool vg0 
test  --указываем volume (имя)
--pool vg0 --указываем в каком пуле удаляем

ЕСЛИ РАБОТА С ВИРТУАЛКОЙ НА УРОВНЕ VITRUALBOX - Т.Е. ДЛЯ ТЕСТИРОВАНИЯ, ЛАБЫ... ТО ЛУЧШЕ ИСПОЛЬЗОВАТЬ ТИП "ДИРЕКТОРИЮ" (COW)

ЕСЛИ ЖЕ ИСПОЛЬЗОВАТЬ "В ПРОМЫШЛЕННЫХ ЦЕЛЯХ", ТО ЛУЧШЕ СОЗДАВАТЬ ПУЛЫ НА ЛОГИЧЕСКОЙ СТРУКТУРЕ

НАСТРОЙКА ХОСТА ЗАКОНЧЕНА

ПОДКЛЮЧАЕМ К ХОСТУ ПУЛ НА ZFS
zpool-list   --смотреть пулч на ZFS

еще раз - удаляем пул
pool-destroy zp3  --переведем пул с именем zp3 в неактивное состояние
pool-undefine zp3  --совсем убираем пул из конфигурации хоста

этот пул - исключен и забыт в виртуальной машине, а физически все еще содержит информацию - ничего не стерлось


ПОДКЛЮЧАЕМ ПУЛ К НАШЕМУ ХОСТУ
pool-define-as --name zp3 --type zfs --tarrget zp3
--name zp3 --имя нашего пула
--type zfs --тип zfs
--tarrget zp3 --указываем, что это за zfs пул, он также у нас имеет имя в системе zfs

смотрим
pool-list --all
наш пул не активен и autostart=no

активируем и дклаем autostart=yes
pool-start zp3
pool-autostart zp3

а теперь смотрим диски
vol-list --pool zp3


СОЗДАНИЕ ВИРТУАЛЬНЫХ МАШИН
virsh - переходим
создаем новый volume для нашей виртуальной машины
vol-create-as --name test1-disk1 --pool zp3 --capasity 8G
--name test1-disk1    --название
--pool zp3   --какой пул используем
--capasity 8G   -- выделяем 8гиг места

создаем еще один диск для второй виртуальной машины
vol-create-as --name test2-disk1 --pool zp3 --capasity 24G

выходим из virsh

СОЗДАЕТСЯ ВИРТУАЛЬНАЯ МАШИНА (bios) ИЗ SHELL
virt-install --name test1 --vcpus socoet=2,cores=2,threads=2 --memory 4096 --network network=default,model=virtio,mac=xxxxxxx --graphics spice --video qxl --disk vol=zp3/test1-disk1,cache=none,format=raw --boot hdd,cdrom --cdrom=/mnt/iso/ubuntu-17.04-desctop-amd64.iso --virt-type kvm --hvm --sound ac97

--name test1  --имя создаваемиой виртуальной машины
--vcpus 1   --сколько процессоров должно быть у машины - простой вариант
socoet=2 --сколько сокетов у машины - двухпроцессорная -после запятой - нет пробела
cores=2 -- сколько ядер  -после запятой - нет пробела
threads=2  --количество потоков (у интел) -после запятой - нет пробела
получаем двухпроцессорную четырехяедрную машину и 2 потока - суммарно нам даст 8 логических процессоров
ДЛЯ МАШИНЫ ДЛЯ WINDOWS - ЖЕЛАТЕЛЬНО УКАЗЫВАТЬ КОЛ-ВО СОКЕТОВ И ЯДЕР ЕСЛИ КОЛ-ВО ВИРТУАЛЬНЫХ ПРОЦЕССОРОВ ОТЛИЧАЕТСЯ ОТ БОЛЕЕ ЧЕМ 2
Т.Е. ЕСЛИ ВЫДЕЛЯЕМ 4 ПРОЦЕССОРА ДЛЯ WINDOWS - ТО ЖЕЛАТЕЛЬНО РАСПИСАТЬ ТАКИМ ОБРАЗОМ - ТАК WINDOWS РАБОТАЕТ ЛУЧШЕ, ДЛЯ LINUX ТАКИХ ОСОБЕННОСТИЙ НЕ ЗАМЕЧЕНО

--memory 4096  -- отдаем 4Гиг памяти (указываем в мегабайтах)
--network network=default  --какая сеть будет использоваться - default - по умолчанию именно она у нас настроена
model=virtio --модель сети (сетевой карты)- указывать дбя windows
model=e1000 --так же модель сетевой карты, тоже распространена
mac=xxxxxxx  --если есть необходимость - можно указать mac, если не указывать - mac присвоится случайным образом
ПОСЛЕ ЗАПЯТОЙ НЕТ ПРОБЕЛА

--graphics spice  --указываем графику, используем для обмена ПРОТОКОЛ SPICE для обмена данными,  порт можно не указывать, он возмется по умолчанию, 
начиная с 5901 (зависит от кол-а виртуальных машин - берется следующий)
SPICE - рекомендуется ставить когда работаем с десктопными операционными системами -в этом случае устанавливается виртуальная графическая 
карта QXL и с windows она работает лучше, чем стандартный виндовый адаптер

--video qxl  --указываем саму видеокарту - лучше его указывать - если не указать - возможны "тормоза"

--disk vol=zp3/test1-disk1,cache=none,format=raw   --указываем наш диск
vol=zp3  -- пул
test1-disk1  --название диска

cache=none  --кэширование - тут отсутствует - не обязательный параметр
format=raw  --raw стоит по умолчанию, но можно прописать явным образом

ЕСЛИ В КАЧЕСТВЕ ДИСКА НУЖНО УКАЗАТЬ ОТДЕЛЬНЫЙ IMAGE ФАЙЛ, ТО ЕМУ НУЖНА ОПЦИЯ PATH И УКАЗАТЬ ДО НЕГО ПУТЬ

--disk path=/var/lib/qemu/test1.img,format=qcow2,size=10
qcow2   --лучше использовать такой формат хранения данных
size=10  --10Гиг - по умолчению тут измерение в Гигабайтах

--boot hdd,cdrom   --указывваем каким образом необходимо загрузчику обходить все наши устройства для определения с какого устройства загружаться
грузимся с диска, если его нет, то с cdrom, после установки системы - загрузка будет происходить с жесткого диска

 --cdrom=/mnt/iso/ubuntu-17.04-desctop-amd64.iso  --наш cdrom - путь до образа устанавливаемой системы

--virt-type kvm   --тип виртуальной машины - будет KVM

--hvm   --значит, что наша машина будет полностью виртуаллизирована

ТИП АРХИТЕКТУРЫ ПРОЦЕССОРА УКАЗЫВАТЬ НЕ БУДЕМ, ПО УМОЛЧАНИЮ СИСТЕМА KVM ВОЗЬМЕТ С ПАРАМЕТРОВ ХОСТА

--sound ac97   --указали звуковую карту ac97, но есть и другие

если ошибок нет - пойдет установка



ТЕПЕРЬ ЗАПУСКАЕМ МАШИНУ С UEFI

virt-install --name test2 --vcpus socoet=2,cores=2,threads=2 --memory 4096 --network network=default,model=virtio,mac=xxxxxxx --graphics spice --video qxl --disk vol=zp3/test2-disk1,cache=none,format=raw,bus=sata --boot hdd,cdrom --cdrom=/mnt/iso/ru-en_win8.1_u3_x86-x64_16in1_activated.iso --virt-type kvm --hvm --sound ac97 --boot uefi

bus=sata  --если устанавлдиваем windows - нужно указывать шину приндительно, иначе при некоторых ситукциях будет выскакивать синий 
экран на некоторых операционных системах семейства windows

--boot uefi  --прараметр для работы с uefi



ЕСЛИ МЫ ЗАКРОЕМ ОКНО ПРИ НЕ ЗАВЕРШЕННОЙ УСТАНОВКЕ - ТО ПОТОМ СМОЖЕМ ПРОДОЛЖИТЬ ЕЕ
virsh
list --all

выдаст нам виртуальные машины и их статус (запущен/остановлен)

list   --без параметров покажет только запущенные машины


ЭТО БЫЛИ СТАНДАРТНЫЕ, ОСНОВНЫЕ ПАРАМЕТРЫ, ДЛЯ БОЛЕЕ ТОНКОЙ НАСТРОЙКИ ЧИТАТЬ HELP  И  MAN





ВСЕ ЭТО МОЖНО СДЕЛАТЬ И ЧЕРЕЗ ГРАФИЧЕСКИЙ ИНТЕРФЕЙС (МЕНЕДЖЕР ВИРТУАЛЬНЫХ МАШИН)


ПОДКЛЮЧЕНИЕ К ВИРТУАЛЬНОЙ МАШИНЕ LINUX
нужно убедиться, что пользователь, под которым мы будем соединяться, присутствует на виртуальной машине и мы по ssh можем к 
нему соединиться (для linux)

ssh metelkov@192.168.122.224

metelkov  --пользователь на виртуальной машине (linux)
@192.168.122.224  --ip адрес машины - смоорим в настройках сети вирт машины

после запросит пароль и мы туда логинимся

groups
после, ОБЯЗАТЕЛЬНО, проверяем группу пользователя, он должен входить в libvirtd -ОБЯЗАТЕЛЬНО

потом консоль можно закрыть

теперь в менеджере вирт машин - файл - добавить соединение - отметить чекбокс "удаленное соединение" - протокол ssh, имя 
пользователя - который у нас заведен на удаленной системе (и входит в группу libvirtd)

узел - указываем ip адр удаленной машины

если нужно ПОДКЛЮЧАТЬСЯ АВТОМАТИЧЕСКИ КАЖДЫЙ РАЗ, КОГДА МЫ ЗАПУСКАЕМ МЕНЕДЖЕР ВИРТУАЛЬНЫХ МАШИН (А НЕ СТАРТУЕМ ХОСТ СИСТЕМУ!)- отмечаем чекбокс

жмем подключиться - идет запрос пароля

после видим, что у нас есть подключение, так же видим ip адр удаленного хоста

СМОТРИМ ЕГО СВОЙСТВА
на нем - правка - свойства подключения  (вирт машина запущена))

закладка ОБЗОР 
на ней видим, что работаем на удаленном хосте, который настраивали

закладка ВИРТУАЛЬНЫЕ СЕТИ
наша сеть по умолчанию (default)
можно посмотреть ip и диапазон dhcp

закладка ПРОСТРАНСТВО ДАННЫХ
покажет наши разделы и куда смонтированы
в этой закладке можно создавать диски для виртуальной машины
ЕСЛИ В LVM НУЖЕН ТОМ - ЖМЕМ НА ПЛЮСИК - вводим наименования тома и указываем размер  (сколько доступно - будет указано)
ТАК ЖЕ И ДЛЯ ПРОСТЫХ ДИСКОВ - так же через плюсик можно добавить, НО Т.К. ЭТО ЯВЛЯЕТСЯ ОБЫЧНЫМ КАТАЛОГОМ В ФАЙЛОВОЙ СИСТЕМЕ, ТО У 
НАС ИЗМЕНИЛОСЬ ВНЕШНЕЕ ПРЕДСТАВЛЕНИЕ - У НОВОГО ДИСКА БУДЕТ РАСШИРЕНИЕ IMG -ЗАДАДИМ НАЗВАНИЕ, НАПРИМЕР, TEST1. ПОТОМ ВЫБИРАЕМ ФОРМАТ, В 
ОТОРОМ БУДУТ ХРАНИТЬСЯ ДАННЫЕ - ВЫБИРАЕМ QCOW2. ТАК ЖЕ УКАЗАТЬ РАЗМЕР (ДОСТУПНОЕ МЕСТО БУДЕТ ПОКАЗАНО)

после в хостовой машине появится и группа lvm - в ней отдельный диск и в смонтированом каталоге /mnt/kvm-disks  - появится файл test1.qcow2

при настройке пулов - мы ставили метку autostart=yes  - тут этот чекбокс отмечен

ТАК ЖЕ В ПРАВОЙ ЧАСТИ ОКНА, В НИЗУ - МОЖНО ДОБАВИТЬ ПУЛ - ЖМЕМ ПЛЮСИК - НАМ ПОКАЖУТ КАКИЕ ВАРИАНТЫ ВЫБОРА ПУЛА ЕСТЬ 
(НЕ ВСЕ МЕНЕДЖЕРЫ ВИРТУАЛЬНЫХ МАШИН РАБОТАЮТ С ZFS, НО ММЕНЕДЖЕР ВИДИТ ZFS ПУЛ - ZFS МОЖНО СОЗДАТЬ ИЗ ШЕЛЛ)

закладка СЕТЕВЫЕ ИНТЕРФЕЙСЫ
указаны настройки сетевых интерфейсов, через которые мы подключаемся
(ИНТЕРФЕЙСЫ ВИРТУАЛЬНОЙ МАШИНЫ)


В МЕНЕДЖЕРЕ ВИРТ МАШИН МОЖНО НАСТРОИТЬ ГРАФИКИ ЗАГРУЗКИ цп, ДИСКА, СЕТИ...
вид - графики - отмечаем чекбоксы и потом настраиваем в ПРАВКА - ПАРАМЕТРЫ
(ПАРАМЕТРЫ - НАСТРОЙКА САМОГО МЕНЕДЖЕРА)
- закладка СТАТИСТИКА - отмечаем чекбоксы которые нам нужны и интервал обновления

-закладка НОВАЯ ВМ - параметры по умаолчанию, котьорые будут использоваться при создании новой вирт машины
тип графики - рекомендовано spice
формат - ставить RAW (почему - см выше)

-закладка ПДТВЕРЖДЕНИЯ
когда мы что либо меняем в параметре менеджера - предупреждение если мы чтото изменили, но принудительно не 
применили (не нажали кнопку "применить" и хотим уйти с вкладки)



ПРИ КОНФИГУРАЦИИ НОВОЙ ВИРТ МАШИНЫ 
- ПРИ ВЫБОРЕ ПРОЦЕССОРА - ЕСЛИ КОЛ-ВО ВИРТУАЛЬНЫХ ПРОЦЕССОРОВ БОЛЬШЕ ЧЕМ 2 - ДЕЛАЕМ ВЫБОР ТОПОЛОГИИ ВРУЧНУЮ
- ДОПУСТИМ 4 ПРОЦЕССОРА - ТО ДЕЛАЕМ СОКЕТОВ = 2, ЯДЕР = 2 (ПОТОК ОСТАВИМ = 1) - ТАК ЛУЧШЕ РАБОТАЕТ ДЛЯ WINDOWS
(ЕСЛИ ВРУЧНУЮ НЕ ВЫБИРАТЬ, А ОСТАВИТЬ КОЛ-ВО ПРОЦЕССОРОВ 4 - ТО В ВИРТ МАШИНЕ ОТОБРАЗИТСЯ 2 ПРОЦЕССОРА  ВСЕГО, НО НУЖНО ЧТОБЫ 
МАКСИМУМ СОВПАДАЛ С ПРОИЗВЕДЕНИЕМ СОКЕТОВ И ЯДРА )
ПОТОКИ ТОЖЕ МОЖНО УСТАНОВИТЬ, НО НЕ ВСЕ ПРОЦЕССОРЫ ПОДДЕРЖИВАЮТ HYPER-THREADING
ПОЭТОМУ ВЫБРАЕМ ДРУГОЙ ПРОЦЕССОР ИЛИ НЕ СВЯЗЫВАЕМСЯ С ПОТОКАМИ И ВЫБИРАЕМ ПО УМОЛЧАНИЮ

НАСТРОЙКА ПАМЯТИ - ЕСЛИ ЗНАЧЕНИЕ МАКСИМУМ УСТАНОВИМ БОЛЬШЕ, ЧЕМ ТЕКУЩЕЕ - ТО ПРИ НЕОБХОДИМОСТИ ХОСТ МОЖЕТ ВЫДЕЛЯТЬ ЕЩЕ ПАМЯТИ (ДО ЗНАЧЕНИЯ МАКСИМУМ)


ПАРАМЕТР VirtIO ДИСК1
там есть возможность выбрать шину диска
для LINUX = VIRTIO
для WINDOWS = SATA (можно еще IDE - для win XP или более старой win)

параметры производительности (обычно не меняем )
параметры кэширования не делаем = none (обычно по умолчанию)
режим ввода-вывода = native

НАСТРОЙКА СЕТИ (ОБЫЧНО DEFAULT)
ЕСЛИ НУЖНО, ЧТОБЫ ВИТР МАШИНА ОКАЗАЛАСЬ В РЕАЛЬНОЙ СЕТИ (В ТОЙ ЖЕ СЕТИ, ЧТО И ХОСТ) - ВЫБИРАЕМ ТУ СЕТЕВУЮ КАРТУ, КОТОРАЯ ПОДКЛЮЧЕНА В СЕТЬ (НА ХОСТ МАШИНЕ),
РЕЖИМ - МОСТ
СЕЙЧАС ВИРТ МАШИНА ДОСТУПНА (МОГУТ ВИДЕТЬ ВСЕ) ДЛЯ ВСЕХ ПОЛЬЗОВАТЕЛЕЙ ИЗ РЕАЛЬНОЙ СЕТИ И МОГУТ К НЕЙ ОБРАЩАТЬСЯ КАК К РЕАЛЬНОЙ МАШИНЕ НО СО 
СВОЕГО ХОСТА СОЕДИНИТЬСЯ С ЭТОЙ ВИРТ МАШИНОЙ УЖЕ НЕ ПОЛУЧИТСЯ
ОБЫЧНО В ЭТОМ НЕТ НЕОБХОДИМОСТИ И ВЫБИРАЕМ NAT - ТОГДА ВИРТ МАШИНА ОБЩАЕТСЯ С ВНЕШНИМ МИРОМ ЧЕРЕЗ NAT

МОДЕЛЬ УСТРОЙСТВА - ОБЫЧНО VIRTIO
ДЛЯ WINDOWS ЛУЧШЕ СТАВИТЬ   E1000

НАСТРОЙКА ВИДЕО
ЖЕЛАТЕЛЬНО СТАВИТЬ QXL - ОСОБЕННО ДЛЯ WINDOWS
CIRRUS ТОЖЕ МОЖНО,   А ОСТАЛЬНЫЕ РАБОТАЮТ НЕ ОЧЕНЬ ХОРОШО






--- НА ДЕБИАН  ----
apt install qemu-kvm libvirt-daemon-system virt-manager ovmf

virt-manager    -- если на десктопной версии убунту
ovmf    --  эмуляция uefi

добавим пользователя в группу libvirt
сначала смотрим, есть ли группа
cat /etc/group | grep libvirt

добавляем в группу пользователя
adduser metelkov libvirt






